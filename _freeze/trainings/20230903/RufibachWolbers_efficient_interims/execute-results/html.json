{
  "hash": "127475e611457d43aac114bb7a0d6e2c",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Advanced group-sequential and adaptive confirmatory clinical trial designs, with R practicals using rpact: Efficient use of futility and efficacy interim analyses in group-sequential designs\"\nauthor: \n- name: \"Kaspar Rufibach\"\n  affiliation: Methods, Collaboration, and Outreach Group (MCO), PD Data and Statistical Sciences, Roche Basel\ndate: 'Last modification: 16 Apr 2024'\nexecute:\n  freeze: auto  # re-render only when source changes\nformat:\n  html:\n    toc: true\n    toc-depth: 3\n    toc-title: Contents\n    toc-location: left\n    number-sections: true\n    number-depth: 3\n    code_download: true\nbibliography: biblio.bib\n---\n\n\n# Purpose of this document\n\nThis R markdown file provides the code accompanying the first theory block in the [CEN 2023](https://cen2023.github.io/home/index.html) pre-conference course [Advanced group-sequential and adaptive confirmatory clinical trial designs, with R practicals using rpact](https://cen2023.github.io/home/courses.html#Advanced_group-sequential_and_adaptive_confirmatory_clinical_trial_designs,_with_R_practicals_using_rpact).\n\n# Setup\n\nLoad rpact and further packages and define shortcut for sample size function.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load rpact\nlibrary(rpact)\nlibrary(knitr)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'knitr'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following object is masked from 'package:rpact':\n\n    kable\n```\n\n\n:::\n\n```{.r .cell-code}\n# shortcut for sample size function, to streamline code\nmyGetSampleSizeSurvival <- function(design){\n    tmp <- getSampleSizeSurvival(design = design,\n                        lambda2 = log(2) / m1, hazardRatio = hr,\n                        dropoutRate1 = dout1, dropoutRate2 = dout2, dropoutTime = douttime,\n                        accrualTime = accrualTime, accrualIntensity = accrualIntensity,\n                        maxNumberOfSubjects = maxNumberOfSubjects)  \n    return(tmp)\n}\n```\n:::\n\n\n# Specify details of example trial\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# design parameters\nalpha <- 0.05\nbeta <- 0.2\nm1 <- 6 * 12\nm2 <- 8 * 12\nhr <- m1 / m2\n\n# constant for computation of variance for log(hr)\n# as a function of P(randomized to A)\npA <- 1 / 2     # 1:1 randomization\nkappa <- (pA * (1 - pA)) ^ (-1)\n\n# timing parameters\ndout1 <- 0.025\ndout2 <- 0.025\ndouttime <- 12\naccrualTime <- 0:6\naccrualIntensity <- seq(6, 42, by = 6)\nmaxNumberOfSubjects <- 1200\n\n# informal futility boundary\ninform_bound <- 1\n```\n:::\n\n\n# How much do we gain with interim analyses?\n\nThe following code allows to compute all the numbers that appear in the first section of the slide deck. We do not show all the numbers here and invite everyone to get them out of the corresponding objects.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Required events without interim\nnevent <- getSampleSizeSurvival(lambda1 = getLambdaByMedian(m2), lambda2 = getLambdaByMedian(m1), \n                                sided = 2, alpha = alpha, beta = beta)\nnevent <- ceiling(nevent$maxNumberOfEvents)\nnevent \n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 380\n```\n\n\n:::\n\n```{.r .cell-code}\n# timing of interims (information fraction)\ni1 <- 0.3\ni2 <- 2 / 3\ninfofrac <- c(i1, i2, 1)\n\n# OBF design without initial interim at 30% (to get cumulative alpha-spending function \n# for these visits) \ndesign_tmp <- getDesignGroupSequential(informationRates = infofrac[-1],\n                                       sided = 1, alpha = alpha / 2, beta = beta,\n                                       typeOfDesign = \"asOF\", futilityBounds = c(-6))\n\ndesign <- getDesignGroupSequential(informationRates = infofrac,\n                                   sided = 1, alpha = alpha / 2, beta = beta,\n                                   futilityBounds = c(log(inform_bound), -6),\n                                   typeOfDesign = \"asUser\", \n                                   userAlphaSpending = c(0.00001, design_tmp$alphaSpent))\n\nsamplesize <- myGetSampleSizeSurvival(design)\n\n# end of recruitment\nrecrend <- samplesize$totalAccrualTime\nrecrend\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 31.57143\n```\n\n\n:::\n\n```{.r .cell-code}\n# total sample size\nnevents_i1 <- as.vector(ceiling(samplesize$eventsPerStage))\nnevent_gs <- max(nevents_i1)\nnevent_gs\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 408\n```\n\n\n:::\n\n```{.r .cell-code}\n# stopping probabilities at futility and efficacy interim under H0 and H1\ndesignChar <- getDesignCharacteristics(design)\nstopProbsH0 <- getPowerAndAverageSampleNumber(design, theta = 0, nMax = designChar$shift)\nstopProbsH1 <- getPowerAndAverageSampleNumber(design, theta = 1, nMax = designChar$shift)\n\nstopFutIA_H0 <- stopProbsH0$earlyStop[\"stage = 1\", 1]\nstopEffIA_H0 <- stopProbsH0$earlyStop[\"stage = 2\", 1]\nc(stopFutIA_H0, stopEffIA_H0)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  stage = 1   stage = 2 \n0.500010000 0.006000121 \n```\n\n\n:::\n\n```{.r .cell-code}\nstopFutIA_H1 <- stopProbsH1$earlyStop[\"stage = 1\", 1]\nstopEffIA_H1 <- stopProbsH1$earlyStop[\"stage = 2\", 1]\nc(stopFutIA_H1, stopEffIA_H1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nstage = 1 stage = 2 \n0.0595379 0.4400694 \n```\n\n\n:::\n\n```{.r .cell-code}\n# Expected number of events under H0 and H1\nexpH0 <- samplesize$expectedEventsH0\nexpH1 <- samplesize$expectedEventsH1\nc(expH0, expH1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 264.3069 331.0536\n```\n\n\n:::\n\n```{.r .cell-code}\n# clinical cutoffs\nmaxNumberOfSubjects1 <- maxNumberOfSubjects / 2\nmaxNumberOfSubjects2 <- maxNumberOfSubjects / 2\nmaxNumberOfSubjects <- maxNumberOfSubjects1 + maxNumberOfSubjects2\n\n# time to interim cutoff\ntime <- 0:100 \n\n# under H0\nprobEventH0 <- getEventProbabilities(time = time, lambda2 = log(2) / m1, \n                                     hazardRatio = 1, dropoutRate1 = dout1, \n                                     dropoutRate2 = dout2, dropoutTime = douttime,\n                                     accrualTime = accrualTime, accrualIntensity = accrualIntensity, \n                                     maxNumberOfSubjects = maxNumberOfSubjects)\nexpEventH0 <- probEventH0$overallEventProbabilities * maxNumberOfSubjects   \ntimefixH0 <- min(time[expEventH0 >= nevent])\ntimeeventsH0 <- c(min(time[expEventH0 >= nevents_i1[1]]), \n                  min(time[expEventH0 >= nevents_i1[2]]), \n                  min(time[expEventH0 >= nevents_i1[3]]))\nexpdurH0 <- timeeventsH0[1] * stopFutIA_H0 + timeeventsH0[2] * stopEffIA_H0 + \n  timeeventsH0[3] * (1 - stopFutIA_H0 - stopEffIA_H0)\n\n# number of events under H0 at end of accrual\nrecrend_H0_n <- floor(min(expEventH0[time >= recrend]))\n\n# under H1\nprobEventH1 <- getEventProbabilities(time = time, lambda2 = log(2) / m1, hazardRatio = hr,\n                                     dropoutRate1 = dout1, dropoutRate2 = dout2, \n                                     dropoutTime = douttime, accrualTime = accrualTime, \n                                     accrualIntensity = accrualIntensity, \n                                     maxNumberOfSubjects = maxNumberOfSubjects)\nexpEventH1 <- probEventH1$overallEventProbabilities * maxNumberOfSubjects   \ntimefixH1 <- min(time[expEventH1 >= nevent])\ntimeeventsH1 <- c(min(time[expEventH1 >= nevents_i1[1]]), \n                  min(time[expEventH1 >= nevents_i1[2]]), \n                  min(time[expEventH1 >= nevents_i1[3]]))\nexpdurH1 <- timeeventsH1[1] * stopFutIA_H1 + timeeventsH1[2] * stopEffIA_H1 + \n  timeeventsH1[3] * (1 - stopFutIA_H1 - stopEffIA_H1)\nc(expdurH0, expdurH1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nstage = 1 stage = 1 \n 46.39165  59.37703 \n```\n\n\n:::\n\n```{.r .cell-code}\n# same design as above but now without futility\ndesign_eff_only <- getDesignGroupSequential(informationRates = infofrac,\n                                   typeOfDesign = \"asOF\", sided = 1, \n                                   alpha = alpha / 2, beta = beta)\n\nsamplesize_eff_only <- myGetSampleSizeSurvival(design_eff_only)\nnevent_eff_only <- ceiling(samplesize_eff_only$maxNumberOfEvents)\n\n# same design as above but now with binding futility interim\ndesign_binding <- getDesignGroupSequential(informationRates = infofrac, \n                                           typeOfDesign = \"asOF\", sided = 1, \n                                           alpha = alpha / 2, beta = beta, futilityBounds = c(0, -6), \n                                           bindingFutility = TRUE)\n\nsamplesize_binding <- myGetSampleSizeSurvival(design_binding)\nnevent_binding <- ceiling(samplesize_binding$maxNumberOfEvents)\nnevent_binding\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 401\n```\n\n\n:::\n:::\n\n\n\n# Optimal use and timing of interim analyses\n\n## Efficacy\n\n### Bias\n\nFor code and recommendationas how to handle bias in group-sequential designs we refer to [Vignette #7](https://www.rpact.com/vignettes/rpact_analysis_examples) on the [rpact vignettes webpage](https://www.rpact.com/vignettes).\n\nThe standard rpact output provides a _median unbiased estimate_. For details on the various types of biases in group-sequential designs and approaches we refer to @wassmer_16_book.\n\n\n### Adding a late efficacy interim\n\nIn what follows, we generate the table used in the slide deck to illustrate addition of a late efficacy interim analysis.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# illustrate effect of late efficacy on MDD and local significance level\n\n# timing of interims (information fraction)\nj1 <- 2 / 3\nj2 <- 0.85\ninfofrac_late <- c(j1, j2, 1)\n\n# OBF standard design\ndesign_late1 <- getDesignGroupSequential(informationRates = infofrac_late[c(1, 3)],\n                                   typeOfDesign = \"asOF\", sided = 2, alpha = alpha, \n                                   beta = beta)\nss_late1 <- myGetSampleSizeSurvival(design_late1)\nnevents_late1 <- ceiling(nevent * c(j1, 1))\n\n# add late interim\ndesign_late2 <- getDesignGroupSequential(informationRates = infofrac_late,\n                                         typeOfDesign = \"asOF\", sided = 2, alpha = alpha, \n                                         beta = beta)\nss_late2 <- myGetSampleSizeSurvival(design_late2)\nnevents_late2 <- ceiling(nevent * c(j1, j2, 1))\n\n# assemble table\ntab_late <- data.frame(matrix(NA, ncol = 5, nrow = 4))\ncolnames(tab_late) <- c(\"\", \"quantity\", paste(\"info = \", \n                                              round(infofrac_late, 2)[1:2], sep = \"\"), \"final\")\ntab_late[c(1, 3), 1] <- paste(\"Design \", 1:2, sep = \"\")\ntab_late[c(1, 3), 2] <- \"MDD\"\ntab_late[c(2, 4), 2] <- \"local significance level\"\ntab_late[1, c(3, 5)] <- round(ss_late1$criticalValuesEffectScaleLower, 3)\ntab_late[2, c(3, 5)] <- format.pval(ss_late1$criticalValuesPValueScale, 3)\ntab_late[3, 3:5] <- round(ss_late2$criticalValuesEffectScaleLower, 3)\ntab_late[4, 3:5] <- format.pval(ss_late2$criticalValuesPValueScale, 3)\nkable(tab_late)\n```\n\n::: {.cell-output-display}\n\n\n|         |quantity                 |info = 0.67 |info = 0.85 |final  |\n|:--------|:------------------------|:-----------|:-----------|:------|\n|Design 1 |MDD                      |0.731       |NA          |0.816  |\n|NA       |local significance level |0.0121      |NA          |0.0463 |\n|Design 2 |MDD                      |0.733       |0.784       |0.813  |\n|NA       |local significance level |0.0121      |0.0265      |0.0404 |\n\n\n:::\n:::\n\n\n\n## Futility\n\nSee backup of this file for the table from the slides.\n\n\n# Backup\n\n## Efficacy\n\n### MDD\n\nMinimal detectable differences can easily be extracted from `getSampleSizeSurvival` objects:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhrMDD <- as.vector(samplesize$criticalValuesEffectScale)\nhrMDD\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.4625061 0.7375959 0.8209002\n```\n\n\n:::\n:::\n\n\n\n\n## Futility\n\n### Conditional power\n\nAs a first approach to determine an interim boundary for futility we reproduce the conditional power plot.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# calculate condition power for interim HR ranging from 0.6 to 1.5\nhrs <- seq(0.6, 1.5, by = 0.01)\ncpower0 <- rep(NA,length(hrs))\ncpower <- cpower0\n\nfor (i in 1:length(hrs)){\n  \n  # generate dataset that contains result up to interim\n  results <- getDataset(\n    overallEvents = nevents_i1[1],\n    overallLogRanks = log(hrs[i]) / sqrt(kappa /nevents_i1[1]),\n    overallAllocationRatio = 1)\n  \n  # proper object that can be used by rpact\n  stageResults <- getStageResults(design, dataInput = results, directionUpper = FALSE)\n  \n  # compute conditional power under H1: theta_1 = 0.75\n  cpower[i] <- getConditionalPower(stageResults, nPlanned = diff(nevents_i1), \n                                   thetaH1 = hr)$conditionalPower[3]\n  \n  # compute conditional power under H0: theta_1 = 1\n  cpower0[i] <- getConditionalPower(stageResults, nPlanned = diff(nevents_i1), \n                                    thetaH1 = 1)$conditionalPower[3]\n}\n\n# what interim effect gives a conditional power of 20%?\ncondpow <- 0.2\nhr_int_cp <- min(hrs[cpower <= condpow])\nhr_int_cp\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 1.28\n```\n\n\n:::\n\n```{.r .cell-code}\n# p-value corresponding to that effect\n# z = log(hr_int_cp) / sqrt(kappa / D_int) --> p = P(N(0, 1) <= z) = 1 - Phi(|z|)\nz <- - log(hr_int_cp) / sqrt(kappa / nevents_i1[1])\np_int_cp <- 1 - pnorm(z)\np_int_cp\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.9144856\n```\n\n\n:::\n\n```{.r .cell-code}\n# check this p-value using rpact:\ndesign_cp <- getDesignGroupSequential(informationRates = infofrac,\n                                   typeOfDesign = \"asOF\", sided = 1, \n                                   alpha = alpha / 2, \n                                   beta = beta,\n                                   futilityBounds = c(z, -6), \n                                   bindingFutility = FALSE)\n\nsamplesize_cp <- myGetSampleSizeSurvival(design_cp)\nsamplesize_cp$futilityBoundsPValueScale[1, 1]\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.9144856\n```\n\n\n:::\n:::\n\n\n[Here](go.roche.com/adaptr/rpactAnalysisExamples.html) you can find more details on conditional power computations in rpact, and also how to switch between different scales, i.e. Z-score, hazard ratio, etc.\n\nAnd now plot the conditional power functions compute above:\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(las = 1, mfrow = c(1, 1), mar = c(4.5, 4.5, 2, 1))\nplot(hrs, cpower, type = \"n\", xlab = expression(\"hazard ratio observed at interim\"),\n     ylab = \"conditional power\", ylim = c(0, 1), axes = FALSE, main = \n       expression(\"CP(\"*theta*\") after futility interim, under treatment effect \"*theta[1]*\" used for powering\"))\naxis(1, at = seq(0.6, 10, by = 0.1))\naxis(2, at = seq(0, 1, by = 0.1))\nabline(v = seq(0.6, 10, by = 0.1), h = seq(0, 1, by = 0.1), col = gray(0.9))\nsegments(0, condpow, hr_int_cp, condpow, lty = 2, col = 3, lwd = 3)\nsegments(hr_int_cp, 0, hr_int_cp, condpow, lty = 2, col = 3, lwd = 3)\nlines(hrs, cpower, col = 2, lwd = 4)\nlines(hrs, cpower0, col = 4, lwd = 4)\nlegend(0.9, 0.9, paste(\"hazard ratio after interim: \", c(hr, 1), sep = \"\"), \n       col = c(2, 4), lwd = 4, bty = \"n\")\n```\n\n::: {.cell-output-display}\n![](RufibachWolbers_efficient_interims_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n### Stopping probabilities\n\nAn alternative way of defining an interim boundary for futility, especially when we use the pivotal Phase 3 with futility interim for the LIP, is to find a **sweet spot** by trading off false-decision probabilities at the interim. To this end, assume\n$$\n  \\hat \\theta \\sim N(\\theta, \\sqrt{4 / d_1}).\n$$\nWe are then interested in the probability of continuation (or stopping, simply one minus) computed as:\n$$\n  P_\\theta(\\hat \\theta \\le \\theta_\\text{int}) \\ = \\ \\Phi\\left(\\frac{\\theta_\\text{int} - \\theta}{\\sqrt{4 / d_1}}\\right),\n$$\nwhere $\\theta_\\text{int}$ is an interim boundary. Below the corresponding plot.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# calculate stopping probabilities for interim HR ranging from 0.6 to 1.5\nhrs2 <- seq(0.6, 1.2, by = 0.01)\n\n# under H0\nstopprob0 <- 1 - pnorm((log(hrs2) - log(1)) / sqrt(kappa / nevents_i1[1]))\n\n# under H1\nstopprob1 <- 1 - pnorm((log(hrs2) - log(hr)) / sqrt(kappa / nevents_i1[1]))\n\n# interim boundary\nsp_bound <- 0.9\nfp <- max((1 - stopprob0)[hrs <= sp_bound]) \nfn <- min(stopprob1[hrs <= sp_bound])\nc(fp, fn)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.2795253 0.1560030\n```\n\n\n:::\n:::\n\n\nWith these quantities, generate the plot.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar(las = 1, mfrow = c(1, 1), mar = c(4.5, 4.5, 2, 4.5))\nplot(hrs2, stopprob0, type = \"n\", xlab = expression(\"interim boundary \"*hat(theta)[int]),\n     ylab = \"\", ylim = c(0, 1), axes = FALSE, main = \"interim stopping probabilities\")\naxis(1, at = seq(0.6, 10, by = 0.1))\nabline(v = seq(0.6, 10, by = 0.05), h = seq(0, 1, by = 0.1), col = gray(0.9))\nlegend(0.75, 1, paste(\"false-\", c(\"negative\", \"positive\"), \": hazard ratio: \", c(hr, 1), sep = \"\"), \n       col = c(2, 4), lwd = 4, bty = \"n\")\n\naxis(2, at = seq(0, 1, by = 0.1), labels = seq(0, 1, by = 0.1), col.axis = 2, line = 0.5)\nmtext(\"false-negative probability\", 2, line = 3, col = 2, las = 3)\naxis(4, at = seq(0, 1, by = 0.1), labels = seq(0, 1, by = 0.1), col.axis = 4, line = 0.5)\nmtext(\"false-positive probability\", 4, line = 3, col = 4, las = 3)\n\nlines(hrs2, stopprob1, col = 2, lwd = 4)\nlines(hrs2, 1 - stopprob0, col = 4, lwd = 4)\n\nsegments(min(hrs2), fn, sp_bound, fn, col = 2, lty = 2, lwd = 4)\nsegments(sp_bound, fn, sp_bound, 0, col = 2, lty = 2, lwd = 4)\n\nsegments(max(hrs2), fp, sp_bound, fp, col = 4, lty = 2, lwd = 4)\n```\n\n::: {.cell-output-display}\n![](RufibachWolbers_efficient_interims_files/figure-html/unnamed-chunk-9-1.png){width=672}\n:::\n:::\n\n\n### $\\beta$-spending\n\nFinally, we illustrate how $\\beta-$-spending can be specified.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# compare designs with no futility vs. a design with beta-spending\n\n# no futility\ndesign0 <- getDesignGroupSequential(sided = 1, alpha = alpha / 2, beta = beta,\n                                        informationRates = infofrac,\n                                        typeOfDesign = \"asOF\", bindingFutility = FALSE)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: 'bindingFutility' (FALSE) will be ignored\n```\n\n\n:::\n\n```{.r .cell-code}\nsamplesize0 <- myGetSampleSizeSurvival(design0)\n\n# beta-spending, non-binding\ndesign_beta <- getDesignGroupSequential(sided = 1, alpha = alpha / 2, beta = beta,\n                                   informationRates = infofrac,\n                                   typeOfDesign = \"asOF\",\n                                   typeBetaSpending = \"bsOF\")\nsamplesize_beta <- myGetSampleSizeSurvival(design_beta)\nnevent_beta <- ceiling(samplesize_beta$maxNumberOfEvents)\n\n# generate table \ntab_beta <- data.frame(matrix(NA, nrow = 11, ncol = 3))\ncolnames(tab_beta) <- c(\"quantity\", \"no futility interim\", \"beta-spending\")\ntab_beta[, 1] <- c(\"number of events\", \n                   \"efficacy boundary 1 (effect size)\", \"efficacy boundary 1 (p-value)\",\n                   \"efficacy boundary 2 (effect size)\", \"efficacy boundary 2 (p-value)\",\n                   \"efficacy boundary 3 (effect size)\", \"efficacy boundary 3 (p-value)\",\n                   \"futility boundary 1 (effect size)\", \"futility boundary 1 (p-value)\",\n                   \"futility boundary 2 (effect size)\", \"futility boundary 2 (p-value)\")\n\ntab_beta[1, 2:3] <- ceiling(c(samplesize0$maxNumberOfEvents, samplesize_beta$maxNumberOfEvents))\ntab_beta[c(2, 4, 6), 2] <- round(samplesize0$criticalValuesEffectScale, 2)\ntab_beta[c(2, 4, 6), 3] <- round(samplesize_beta$criticalValuesEffectScale, 2)\n\ntab_beta[c(2, 4, 6) + 1, 2] <- round(samplesize0$criticalValuesPValueScale, 2)\ntab_beta[c(2, 4, 6) + 1, 3] <- round(samplesize_beta$criticalValuesPValueScale, 2)\n\ntab_beta[c(8, 10), 3] <- round(samplesize_beta$futilityBoundsEffectScale, 2)\ntab_beta[c(9, 11), 3] <- format.pval(samplesize_beta$futilityBoundsPValueScale, 2)\nkable(tab_beta, align = \"lrr\")\n```\n\n::: {.cell-output-display}\n\n\n|quantity                          | no futility interim| beta-spending|\n|:---------------------------------|-------------------:|-------------:|\n|number of events                  |              385.00|           419|\n|efficacy boundary 1 (effect size) |                0.48|           0.5|\n|efficacy boundary 1 (p-value)     |                0.00|             0|\n|efficacy boundary 2 (effect size) |                0.73|          0.74|\n|efficacy boundary 2 (p-value)     |                0.01|          0.01|\n|efficacy boundary 3 (effect size) |                0.82|          0.82|\n|efficacy boundary 3 (p-value)     |                0.02|          0.02|\n|futility boundary 1 (effect size) |                  NA|          1.09|\n|futility boundary 1 (p-value)     |                  NA|          0.68|\n|futility boundary 2 (effect size) |                  NA|          0.87|\n|futility boundary 2 (p-value)     |                  NA|          0.12|\n\n\n:::\n:::\n\n\nWe see that by adding two futility interims based on $\\beta$-spending, we increase the maximal number of events from 385 to `tab_beta[1, 3]'. To compute the power loss of adding the futilities, _conservatively_ assuming they will be adhered to, we compute the power of the design _with_ futilities using the number of events of the design _without_ futilities.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# power of beta-spending design at the number of events without beta-spending\npower <- getPowerSurvival(design_beta, \n                          maxNumberOfEvents = ceiling(samplesize0$maxNumberOfEvents),\n                          maxNumberOfSubjects = maxNumberOfSubjects,\n                          lambda2 = log(2) / m1, hazardRatio = hr,\n                          dropoutRate1 = dout1, dropoutRate2 = dout2, dropoutTime = douttime,\n                          accrualTime = accrualTime, accrualIntensity = accrualIntensity,\n                          directionUpper = FALSE)\n\n# power, as compared to the specified 80%\npower$overallReject\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.7664614\n```\n\n\n:::\n:::\n\n\n### Power loss\n\nFinally, we specify the power loss of adding the various futility boundaries. To this, we proceed as follows:\n\n1. Generate a set of trials with hazard ratio at interim and final, without any interim analysis stopping. The nice thing about rpact is that we can still add `informationRates`, i.e. we get a set of datasets that simulate trials until the prespecified maximal number of events, and these simulation datasets contain the hazard ratio estimates at the time when we have reached `informationRates`% of events. \n\n2. From these datasets we can then extract those that jump over the interim boundary and are significant at the end. Simply computing their proportion with respect to the number of simulations gives an estimate of the power.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# generate a set of trials with HR at interim and final, without futility interim stopping\ndesign_sim <- getDesignGroupSequential(informationRates = infofrac[c(1, 3)],\n                                   sided = 1, alpha = alpha / 2, \n                                   beta = beta,\n                                   typeOfDesign = \"asUser\",\n                                   userAlphaSpending = c(0, 0.025),\n                                   futilityBounds = -6)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nChanged type of design to 'noEarlyEfficacy'\n```\n\n\n:::\n\n```{.r .cell-code}\nsamplesize_sim <- myGetSampleSizeSurvival(design_sim)\n\nnsim <- 10 ^ 4\nsimulationResult <- \n  getSimulationSurvival(design_sim, \n                        lambda2 = log(2) / m1, hazardRatio = hr,\n                        dropoutRate1 = dout1, dropoutRate2 = dout2, dropoutTime = douttime,\n                        accrualTime = accrualTime, accrualIntensity = accrualIntensity,\n                        maxNumberOfSubjects = maxNumberOfSubjects,\n                        plannedEvents = as.vector(ceiling(samplesize_sim$eventsPerStage)),\n                        directionUpper = FALSE, maxNumberOfIterations = nsim,\n                        maxNumberOfRawDatasetsPerStage = 1, seed = 2)\n\n# get aggregate datasets from all simulation runs\naggregateSimulationData <- getData(simulationResult)\n\n# power taking futility into account is proportion of significant trials that ran to the end\n# use MDD from initial design with efficacy interim for final analysis\nhrs_interim <- subset(aggregateSimulationData, stageNumber == 1, select = \"hazardRatioEstimateLR\")\nhrs_final <- subset(aggregateSimulationData, stageNumber == 2, select = \"hazardRatioEstimateLR\")\n\n# now assess power loss for the two interim boundaries we discuss\n\n# futility interim analysis informal boundary of 1\nsurvive_interim <- (hrs_interim <= inform_bound)\nsurvive_final <- (hrs_final <= samplesize$criticalValuesEffectScale[3])\nloss_inform <- mean(survive_interim & survive_final)\n\n# futility interim analysis based on conditional power\nsurvive_interim <- (hrs_interim <= hr_int_cp)\nsurvive_final <- (hrs_final <= samplesize$criticalValuesEffectScale[3])\nloss_cp <- mean(survive_interim & survive_final)\n\n# stopping probabilities\nsurvive_interim <- (hrs_interim <= sp_bound)\nsurvive_final <- (hrs_final <= samplesize$criticalValuesEffectScale[3])\nloss_sp <- mean(survive_interim & survive_final)\n\n# power loss from beta-spending design\npl_spending <- power$overallReject\n\n# generate output table\ntab_pl <- data.frame(matrix(NA, ncol = 2, nrow = 4))\ncolnames(tab_pl) <- c(\"boundary\", \"power\")\nrownames(tab_pl) <- c(\"Design 1 (informal)\", \"Design 2 (conditional power)\", \n                      \"Design 3 (stopping probabilities)\", \"Design 4 (beta-spending)\")\ntab_pl[, 1] <- round(c(inform_bound, hr_int_cp, sp_bound, NA), 2)\ntab_pl[, 2] <- round(c(loss_inform, loss_cp, loss_sp, 1 - design_beta$beta), 2)\ntab_pl[tab_pl == \"NA\"] <- \"\"\nkable(tab_pl)\n```\n\n::: {.cell-output-display}\n\n\n|                                  | boundary| power|\n|:---------------------------------|--------:|-----:|\n|Design 1 (informal)               |     1.00|  0.78|\n|Design 2 (conditional power)      |     1.28|  0.80|\n|Design 3 (stopping probabilities) |     0.90|  0.72|\n|Design 4 (beta-spending)          |       NA|  0.80|\n\n\n:::\n:::\n\n\n# MIRROS\n\nThe original MIRROS publication is available [here](https://onlinelibrary.wiley.com/doi/full/10.1002/pst.1969). The accompanying code is available on github [here](https://github.com/numbersman77/integratePhase2).  \n\nThe Bayesian predictive power computations after not stopping at an interim based on point or interval knowledge are described [in this publication](https://www.tandfonline.com/doi/full/10.1080/10543406.2014.972508). The corresponding R package is [bpp](https://cran.r-project.org/package=bpp), available on [CRAN](https://cran.r-project.org/).\n\n# References",
    "supporting": [
      "RufibachWolbers_efficient_interims_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}